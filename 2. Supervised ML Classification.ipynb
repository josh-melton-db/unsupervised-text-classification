{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76a90fae-ea9c-4a6b-923b-243e4580e510",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run using ML Runtime Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfc5150b-2f55-4db1-9f8f-9b36d2951dad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configurations"
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"main\"\n",
    "schema = \"default\"\n",
    "labeled_table = f\"{catalog}.{schema}.labeled_pilot_notes\"\n",
    "model_name = f\"{catalog}.{schema}.pilot_notes_model\"\n",
    "target_table = f\"{catalog}.{schema}.pilot_notes_supervised_classification\"\n",
    "\n",
    "LABEL_COL = \"unsupervised_prediction\"\n",
    "TEXT_COL = \"pilot_notes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f4ae9b6-e66d-4673-99af-4d621263f7c1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read Data"
    }
   },
   "outputs": [],
   "source": [
    "labeled_df = spark.read.table(labeled_table).dropna(subset=[TEXT_COL, LABEL_COL])\n",
    "train_df, test_df = labeled_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62f8831f-a4ed-42d4-aec9-ead595076456",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define SparkML Pipeline"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer, IndexToString\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# 1) Supervised label -> index\n",
    "label_indexer = StringIndexer(inputCol=LABEL_COL, outputCol=\"label\", handleInvalid=\"skip\")\n",
    "label_indexer_model = label_indexer.fit(train_df)  \n",
    "\n",
    "# 2) Text -> tokens -> filtered tokens\n",
    "regex_tok = RegexTokenizer(inputCol=TEXT_COL, outputCol=\"tokens\", pattern=\"\\\\W+\", minTokenLength=2)\n",
    "stop_rem = StopWordsRemover(inputCol=\"tokens\", outputCol=\"tokens_no_sw\")\n",
    "\n",
    "\n",
    "# 3) Vectorize + TF-IDF\n",
    "cv = CountVectorizer(inputCol=\"tokens_no_sw\", outputCol=\"tf\", vocabSize=100_000, minDF=2)\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"features\")\n",
    "\n",
    "\n",
    "# 4) Classifier (use whatever you prefer here)\n",
    "clf = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=50, regParam=0.0, elasticNetParam=0.0)\n",
    "\n",
    "\n",
    "# 5) Optional: recover human-readable predicted label for convenience\n",
    "label_to_str = IndexToString(\n",
    "    inputCol=\"prediction\",\n",
    "    outputCol=\"predicted_label\",\n",
    "    labels=label_indexer_model.labels\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[label_indexer_model, regex_tok, stop_rem, cv, idf, clf, label_to_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ca04de6-5c4d-408a-8463-650b931d8aa8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define Hyperparameter Search"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(cv.vocabSize, [50_000, 100_000])\n",
    "    .addGrid(clf.regParam, [0.0, 0.01, 0.1])\n",
    "    .addGrid(clf.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "cv_estimator = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=4,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3967dd4-fb95-4f4f-a896-90369f957b8e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train, Log, and Register Model"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=\"sparkml_pipeline_cv\"):\n",
    "\n",
    "    # Fit CV; this returns a CrossValidatorModel whose .bestModel is a PipelineModel\n",
    "    cv_model = cv_estimator.fit(train_df)\n",
    "    best_pipeline_model = cv_model.bestModel\n",
    "\n",
    "    # Evaluate on test\n",
    "    test_pred = best_pipeline_model.transform(test_df)\n",
    "    test_acc = evaluator.evaluate(test_pred)\n",
    "\n",
    "    # Useful extra metrics\n",
    "    f1_eval = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    "    )\n",
    "    test_f1 = f1_eval.evaluate(test_pred)\n",
    "\n",
    "    mlflow.log_metric(\"test_accuracy\", test_acc)\n",
    "    mlflow.log_metric(\"test_f1\", test_f1)\n",
    "\n",
    "    # Infer model signature from input/output schema\n",
    "    sample_input = train_df.limit(5).toPandas()\n",
    "    sample_output = best_pipeline_model.transform(train_df.limit(5)).toPandas()[[\"prediction\"]]\n",
    "    signature = infer_signature(sample_input, sample_output)\n",
    "\n",
    "    # Log the entire PipelineModel as one artifact and register it in Unity Catalog\n",
    "    registered_model = mlflow.spark.log_model(\n",
    "        spark_model=best_pipeline_model,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=model_name,\n",
    "        signature=signature,\n",
    "        input_example=sample_input\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aec5abb-bed6-4f2d-bdf0-1ace4d219f2f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Model, Save Predictions"
    }
   },
   "outputs": [],
   "source": [
    "model_version = registered_model.registered_model_version\n",
    "loaded_model = mlflow.spark.load_model(f\"models:/{model_name}/{model_version}\")\n",
    "predictions = loaded_model.transform(labeled_df)\n",
    "predictions.write.mode(\"overwrite\").saveAsTable(target_table)\n",
    "spark.read.table(target_table).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00887e71-a0a8-4446-b563-39d0a2a0da6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2. Supervised ML Classification",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
