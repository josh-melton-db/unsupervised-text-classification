{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76a90fae-ea9c-4a6b-923b-243e4580e510",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run using ML Runtime Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfc5150b-2f55-4db1-9f8f-9b36d2951dad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configurations"
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"main\"\n",
    "schema = \"default\"\n",
    "labeled_table = f\"{catalog}.{schema}.labeled_pilot_notes\"\n",
    "model_name = f\"{catalog}.{schema}.pilot_notes_model\"\n",
    "target_table = f\"{catalog}.{schema}.pilot_notes_supervised_classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e73d82ab-d34b-4e54-9e58-08fc45261ec1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create NLP Features"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import *\n",
    "\n",
    "\n",
    "labeled_df = spark.read.table(labeled_table)\n",
    "normalized_df = (\n",
    "    labeled_df\n",
    "    .withColumn(\"pilot_notes\", lower(col(\"pilot_notes\")))\n",
    "    .withColumn(\"pilot_notes\", regexp_replace(col(\"pilot_notes\"), \"_[0-9]+_\", \" \"))\n",
    ")\n",
    "tokenizer = RegexTokenizer(inputCol=\"pilot_notes\", outputCol=\"words\", pattern=\"\\\\W\") # Can remove stopwords, use synonyms, etc here too\n",
    "tokenized_df = tokenizer.transform(normalized_df)\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"raw_features\", vocabSize=20000, minDF=5)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "print(\"Fitting CV...\")\n",
    "cv_model = cv.fit(tokenized_df)\n",
    "df_cv = cv_model.transform(tokenized_df).cache()\n",
    "\n",
    "print(\"Fitting IDF...\")\n",
    "idf_model = idf.fit(df_cv)\n",
    "features_df = idf_model.transform(df_cv).cache()\n",
    "\n",
    "label_indexer = StringIndexer(inputCol=\"unsupervised_prediction\", outputCol=\"label\")\n",
    "label_model = label_indexer.fit(features_df)\n",
    "labeled_df = label_model.transform(features_df).cache()\n",
    "\n",
    "train_data, test_data = labeled_df.randomSplit([0.8, 0.2], seed=42)\n",
    "train_data = train_data.cache()\n",
    "test_data = test_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b730efc-8827-4004-8be5-16e4478e2095",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train Model Function"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.types import DoubleType\n",
    "from mlflow.types import ColSpec, DataType, Schema\n",
    "from mlflow.models.signature import ModelSignature\n",
    "import mlflow\n",
    "\n",
    "mlflow.autolog()\n",
    "\n",
    "def train_model(model_class, params, run_name):\n",
    "    with mlflow.start_run(run_name=run_name, nested=True):\n",
    "        print(f\"\\nTraining {run_name}...\")\n",
    "        model = model_class(**params)\n",
    "        pipeline = Pipeline(stages=[model])\n",
    "\n",
    "        # Minimal tuning for speed\n",
    "        param_grid = ParamGridBuilder().build() \n",
    "        if isinstance(model, RandomForestClassifier):\n",
    "            param_grid = ParamGridBuilder() \\\n",
    "                .addGrid(model.numTrees, [50, 100]) \\\n",
    "                .build()\n",
    "\n",
    "        evaluator = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "        cv = CrossValidator(estimator=pipeline,\n",
    "                            estimatorParamMaps=param_grid,\n",
    "                            evaluator=evaluator,\n",
    "                            numFolds=2,\n",
    "                            parallelism=2)\n",
    "\n",
    "        cv_model = cv.fit(train_data)\n",
    "        predictions = cv_model.transform(test_data)\n",
    "\n",
    "        accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "        mlflow.log_metric(\"test_accuracy\", accuracy)\n",
    "        return cv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5ff3eed-eaa8-4718-b1e1-a291373440ef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Random Forest"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf_model = train_model(\n",
    "    RandomForestClassifier,\n",
    "    {\"numTrees\": 100, \"maxDepth\": 10},\n",
    "    \"RandomForest_v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "155c951e-9e82-4db3-8d37-143a5ad5c25f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Logistic Regression"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr_model = train_model(\n",
    "    LogisticRegression,\n",
    "    {\"maxIter\": 50, \"elasticNetParam\": 0.5},\n",
    "    \"LogReg_v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbd38fba-1cb7-455d-bfd5-07330ece2771",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Naive Bayes"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb_model = train_model(\n",
    "    NaiveBayes,\n",
    "    {\"smoothing\": 1.0},\n",
    "    \"NaiveBayes_v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ddcac5a-cffb-42a2-9a4f-7e860fbbcd6f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Use Model for Inference"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "best_run = mlflow.search_runs(\n",
    "    order_by=['metrics.test_accuracy DESC', 'start_time DESC'],\n",
    "    max_results=1\n",
    ").iloc[0]\n",
    "model_uri = f\"runs:/{best_run.run_id}/model\"\n",
    "loaded_model = mlflow.spark.load_model(model_uri)\n",
    "predictions_df = loaded_model.transform(labeled_df)\n",
    "\n",
    "label_converter = IndexToString(inputCol=\"prediction\", outputCol=\"supervised_prediction\", labels=label_model.labels)\n",
    "labels_df = label_converter.transform(predictions_df).drop(\"prediction\")\n",
    "labels_df.write.saveAsTable(target_table)\n",
    "spark.read.table(target_table).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2. Supervised ML Classification",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
